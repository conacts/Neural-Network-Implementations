# Neural Network Implementations

This repository contains separate implementations of neural network models using PyTorch. Each implementation focuses on a different topic or task. The following is a brief description of each tutorial:

## GAN with MNIST Dataset
This tutorial implements a Generative Adversarial Network (GAN) using PyTorch to generate realistic-looking images of handwritten digits from the MNIST dataset. The GAN architecture consists of a generator and a discriminator network trained in an adversarial manner.

## Self-Attention and the Vision Transformer (ViT)
In this tutorial, a self-attention mechanism and the Vision Transformer (ViT) architecture are implemented using PyTorch. The ViT model applies self-attention to images, achieving state-of-the-art results in various vision tasks. The tutorial explores the ViT architecture and its application to image recognition.

## Self-Supervised Contrastive Learning
This tutorial focuses on self-supervised contrastive learning, a technique used for unsupervised learning without labels. The implementation uses PyTorch to train a model using contrastive learning to cluster images and their augmented versions in a latent space. The SimCLR method is introduced and implemented as an example of contrastive learning.

## Image Captioning using CNN and LSTM
In this assignment, an image captioning model is implemented using convolutional neural networks (CNNs) and recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM), with PyTorch. The model takes images as input and generates natural language descriptions for them. The Microsoft COCO dataset is used for training and evaluation.

## Implementing a Neural Network in NumPy
This homework assignment focuses on implementing a simple neural network using NumPy. The goal is to gain practice with Python and NumPy while building intuition about the forward and backward propagation algorithms. The implementation follows a basic neural network architecture and includes parameter initialization, cost function calculation, and gradient descent optimization.
